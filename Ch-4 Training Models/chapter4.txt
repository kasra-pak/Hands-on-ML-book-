Two different ways to train a Linear Regression model:
  1. Using a direct "closed-form" equation that directly computes the model parameters that best fit
     the model to training set.
  2. Using an iterative optimization approach called Gradient Descent(GD) that gradually tweaks the 
     model parameters to minimize the cost function over the training set. We will look at a few
     variants of Gradient Descent : Batch GD, Mini-batch GD, and Stochastic GD.

The Second way is better suited for cases where there are a large number of features or too many
training instances to fit in memory.

An important parameter in Gradient Descent is the size of the steps, determined by the learning rate
hyperparameter.

When using Gradient Descent, you should ensure that all features have a similar scale (e.g., using
Scikit-Learn’s StandardScaler class), or else it will take much longer to converge.

Stochastic Gradient Descent has a better chance of finding the global minimum than Batch Gradient
Descent does.

Next we will look at Polynomial Regression, a more complex model that can fit nonlinear datasets. 
Since this model has more parameters  Linear Regression, it is more prone to overfitting the training set.

Surprisingly, you can use a linear model to fit nonlinear data. A simple way to do this is to add
powers of each feature as new features, then train a linear model on this extended set of features.
This technique is called Polynomial Regression.

How to determine the complexity of your model, whether it is overfitting or underfitting:
  1. cross-validation
  2. learning curves
  
A model's generalization error can be expressed as the sum of three different errors:
  *Bias: is due to wrong assumptions; such as assuming that the data is linear when it is actually
         quadratic.
  *Variance: due to the model's excessive sensitivity to small variations in the training data.
  *Irreducible error: due to noisiness of the data itself.

<--Regularized Linear Models-->
For a polynomial model the best way would be to reduce the number of polynomial degrees.
For a linear model, regularization is typically achieved by constraining the weights of the model.
We will look at Ridge Regression, Lasso Regression and Elastic Net.

<-Ridge Regression->
(also called Tikhonov regularization)
a regularization term is added to the cost function. This forces the learning algorithm to not only
fit the data but also keep the model weights as small as possible.

Note that the regularization term should only be added to the cost function during training.
Once the model is trained, you want to use the unregularized performance measure to evaluate the
model’s performance.

<-Lasso Regression->
Least Absolute Shrinkage and Selection Operator Regression (usually simply called Lasso Regression)
just like Ridge Regression, it adds a regularization term to the cost function, but it uses the ℓ1
norm of the weight vector instead of half the square of the ℓ2 norm.

An important characteristic of Lasso Regression is that it tends to eliminate the weights of the
least important features (i.e., set them to zero)
