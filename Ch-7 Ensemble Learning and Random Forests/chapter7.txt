Most popular Ensemble methods: bagging, boosting, stacking, Random Forrests.

<--Voting Classifiers-->
Suppose you have trained a few classifiers, each one achieving about 80% accuracy. A very simple way to 
create an even better classifier is to aggregate the predictions of each classifier and predict the
class that gets the most votes. This majority-vote classifier is called a hard voting classifier.

If all classifiers are able to estimate class probabilities (i.e., they all have a predict_proba() 
method), then you can tell Scikit-Learn to predict the class with the highest class probability, 
averaged over all the individual classifiers. This is called soft voting.

<--Bagging and Pasting-->
One way to get a diverse set of classifiers is to use very different training algorithms, as just 
discussed. Another approach is to use the same training algorithm for every predictor and train them on
different random subsets of the training set. When sampling is performed with replacement, this method 
is called bagging (short for bootstrap aggregating ). When sampling is performed without replacement, it 
is called pasting.

  sklearn.ensemble.BaggingClassifier(dt, n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)
  for bagging set bootstrap to True and for pasting set it to False.

The BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier
can estimate class probabilities (i.e., if it has a predict_proba() method), which is the case with
Decision Tree classifiers.

<--Out-of-Bag Evaluation-->
With bagging, some instances may be sampled several times for any given predictor, while others may 
not be sampled at all. By default a BaggingClassifier samples m training instances with replacement 
(bootstrap=True), where m is the size of the training set. This means that only about 63% of the
training instances are sampled on average for each predictor. The remaining 37% of the training
instances that are not sampled are called out-of-bag (oob) instances. Note that they are not the same 
37% for all predictors.

Since a predictor never sees the oob instances during training, it can be evaluated on these 
instances, without the need for a separate validation set. You can evaluate the ensemble itself by
averaging out the oob evaluations of each predictor.

In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to request an automatic 
oob evaluation after training.

<--Random Patches and Random Subspaces-->
The BaggingClassifier class supports sampling the features as well as instances
  hyperparameters for feature sampling: max_features, bootstrap_features
  hyperparameters for instance sampling: max_samples, bootstrap

feature sampling is useful when dealing with high-dimensional inputs such as images.

Sampling both training instances and features is called the Random Patches method. Keeping all 
training instances (by setting bootstrap=False and max_samples=1.0) but sampling features (by setting 
bootstrap_features to True and/or max_features to a value smaller than 1.0) is called the Random 
Subspaces method.

<--Random Forests-->
Extra-Trees:
It is possible to make trees even more random by also using random thresholds for each feature rather 
than searching for the best possible thresholds. A forest of such extremely random trees is called an 
Extremely Randomized Trees ensemble (or Extra-Trees for short).
  sklearn.ensemble.ExtraTreesClassifier

<--Feature Importance-->
Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that 
feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted 
average, where each node’s weight is equal to the number of training samples that are associated with 
it. Scikit-Learn computes this score automatically for each feature after training, then it scales
the results so that the sum of all importances is equal to 1.

<--Boosting-->
Boosting (originally called hypothesis boosting) refers to any Ensemble method that can combine
several weak learners into a strong learner. The general idea of most boosting methods is to train
predictors sequentially, each trying to correct its predecessor. There are many boosting methods 
available, but by far the most popular are AdaBoost (short for Adaptive Boosting) and Gradient 
Boosting.

<--AdaBoost-->
For example, when training an AdaBoost classifier, the algorithm first trains a base classifier (such
as a Decision Tree) and uses it to make predictions on the training set. The algorithm then increases 
the relative weight of misclassified training instances. Then it trains a second classifier, using 
the updated weights, and again makes predictions on the training set, updates the instance weights, 
and so on.

There is one important drawback to this sequential learning technique: it cannot be parallelized

Scikit-Learn uses a multiclass version of AdaBoost called SAMME (which stands for Stagewise Additive 
Modeling using a Multiclass Exponential loss function). When there are just two classes, SAMME is 
equivalent to AdaBoost.

<--Gradient Boosting-->
instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to 
fit the new predictor to the residual errors made by the previous predictor.

sklearn.ensemble.GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0

The learning_rate hyperparameter scales the contribution of each tree. If you set it to a low value,
such as 0.1, you will need more trees in the ensemble to fit the training set, but the predictions
will usually generalize better. This is a regularization technique called shrinkage.

<--Stacking-->
Or stacked generalization

It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate 
the predictions of all predictors in an ensemble, why don’t we train a model to perform this 
aggregation? we call this last model blender or meta learner.
